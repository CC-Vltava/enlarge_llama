在使用交叉熵验证的时候，自动忽略label=-100的位置，所以可以将padding位置设为-100

在存储模型的时候，如果对embedding matrix进行resize的话，一定要小心，因为读取数据的时候会出错！

tokenizer(
    prompt, 
    return_tensors='pt',
    max_length=256, 
    truncation=True,
    padding='longest'
)
这个里面padding有:
longest
max_length
返回中的attention很重要

Tokenize部分
在进行tokenizer的时候，如果使用max_length导致加入了额外的padding，并且没有在attention中进行处理，就会让结果出现问题

tokenizer的时候，如果长度超出的时候，max_length增加额外padding是不会影响原有tokenize结果的

（有时候）对于llama模型在generate的时候，生成的generate_ids前半段会将原有的输入信息输出，后半段才会输出生成信息

Embedding部分
在进行embedding的时候，如果tokenize结果中含有padding，不会影响原有语句的embedding结果（不管是left还是right添加padding）
对于同一组数据以及同样的weight，每一次读取进来的模型，会有不同的结果（前面的数据基本一样，最后padding部分可能会有一点点偏差）

Model部分
在模型进行特征提取中，不管是普通模型还是对话模型，只要attention为1的部分的信息一样，那么输出就肯定是一样的
不管是last_hidden_state还是logits
所以在generate部分所出现的差异，很有可能只是因为generate的时候还是用了其他的算法
如果自己手动在外面先进行embedding，输入之后得到的结果是一样的（和Generate进行区别）

Generate部分
在generate的时候，直接输入input_ids和直接embedding之后在输入，会影响输出结果
(这个和generate里面对于embedding生成有关，不是llama模型里面直接生成)

在generate的时候，每次读入模型不一样，会导致输出不一样；只要没有重新读入模型，同样输入就不会得到不同结果
这个结果有时候差异还很大！！！

BOS和EOS一定要加上！！！

输入的长度（padding+attention）仍然会对于结果造成影响（😓

LlamaForCausalLM模型不会返回hidden_states
BaseModel有last_hidden_states
